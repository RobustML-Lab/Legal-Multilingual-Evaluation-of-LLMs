{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:14:50.936772Z",
     "start_time": "2024-09-14T17:14:47.854532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Define label options (21 categories)\n",
    "label_options = [\n",
    "    \"POLITICS\", \"INTERNATIONAL RELATIONS\", \"EUROPEAN UNION\", \"LAW\", \"ECONOMICS\",\n",
    "    \"TRADE\", \"FINANCE\", \"SOCIAL QUESTIONS\", \"EDUCATION AND COMMUNICATIONS\", \"SCIENCE\",\n",
    "    \"BUSINESS AND COMPETITION\", \"EMPLOYMENT AND WORKING CONDITIONS\", \"TRANSPORT\",\n",
    "    \"ENVIRONMENT\", \"AGRICULTURE, FORESTRY AND FISHERIES\", \"AGRI-FOODSTUFFS\",\n",
    "    \"PRODUCTION, TECHNOLOGY AND RESEARCH\", \"ENERGY\", \"INDUSTRY\", \"GEOGRAPHY\",\n",
    "    \"INTERNATIONAL ORGANISATIONS\"\n",
    "]"
   ],
   "id": "40a0c66ff49d6be1",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:14:24.254741Z",
     "start_time": "2024-09-14T17:14:21.641887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the Multi-EURLEX dataset (assuming English subset)\n",
    "dataset = load_dataset('multi_eurlex', 'all_languages', split='test')"
   ],
   "id": "e8c0102d5b4dca76",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:14:24.974636Z",
     "start_time": "2024-09-14T17:14:24.962393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to prompt BART with text and label options\n",
    "def classify_text_with_bart(text, label_options):\n",
    "    # Prepare the input for BART\n",
    "    prompt = f\"={text}\\n\\n DO NOT REPEAT THE TEXT. Select the most relevant categories from the list below. Only list the categories that best match the content of the text. DO NOT REPEAT THE ENTIRE LIST, just give a concise answer:\\n\\nCategories: {', '.join(label_options)}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate output from BART\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=200, num_beams=5, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Extract relevant labels from BART's output\n",
    "def extract_labels_from_generated_text(generated_text, label_options):\n",
    "    relevant_labels = []\n",
    "    for label in label_options:\n",
    "        if label.lower() in generated_text.lower():\n",
    "            relevant_labels.append(label)\n",
    "    return relevant_labels\n",
    "\n",
    "# Function to map label names back to numbers\n",
    "def map_labels_to_indices(label_names, label_options):\n",
    "    label_indices = [label_options.index(label) for label in label_names if label in label_options]\n",
    "    return label_indices\n",
    "\n",
    "# Function to preprocess the dataset and map true labels to category numbers\n",
    "def preprocess_dataset(dataset, label_options):\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for item in dataset:\n",
    "        text = item['text']['en']  # Extract English text\n",
    "        labels = item['labels']    # True label numbers\n",
    "\n",
    "        preprocessed_data.append({\"text\": text, \"labels\": labels})\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Evaluate the model on the entire dataset\n",
    "def evaluate_bart_on_dataset(dataset, label_options):\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        text = entry['text']\n",
    "        true_labels = entry['labels']\n",
    "\n",
    "        # Get BART's generated labels\n",
    "        generated_text = classify_text_with_bart(text, label_options)\n",
    "        predicted_label_names = extract_labels_from_generated_text(generated_text, label_options)\n",
    "        predicted_labels = map_labels_to_indices(predicted_label_names, label_options)\n",
    "\n",
    "        # Store true and predicted labels for later comparison\n",
    "        all_true_labels.append(true_labels)\n",
    "        all_predicted_labels.append(predicted_labels)\n",
    "\n",
    "        print(f\"\\nText: {generated_text}\")\n",
    "        print(f\"Generated labels: {predicted_label_names}\")\n",
    "        print(f\"True labels: {true_labels}, Predicted labels: {predicted_labels}\")\n",
    "\n",
    "    return all_true_labels, all_predicted_labels"
   ],
   "id": "2a97608e606c4e9b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:14:29.064046Z",
     "start_time": "2024-09-14T17:14:25.830849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the dataset\n",
    "preprocessed_data = preprocess_dataset(dataset, label_options)\n",
    "\n",
    "# Run the evaluation on the entire preprocessed dataset\n",
    "true_labels, predicted_labels = evaluate_bart_on_dataset(preprocessed_data, label_options)\n",
    "\n",
    "# Now you can compare true_labels and predicted_labels to compute accuracy, precision, etc.\n",
    "# Flatten the lists of true and predicted labels for evaluation\n",
    "flattened_true = [label for sublist in true_labels for label in sublist]\n",
    "flattened_pred = [label for sublist in predicted_labels for label in sublist]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(flattened_true, flattened_pred, average='macro')\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ],
   "id": "c364e3a32aee420e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 902, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m preprocessed_data \u001B[38;5;241m=\u001B[39m preprocess_dataset(dataset, label_options)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Run the evaluation on the entire preprocessed dataset\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m true_labels, predicted_labels \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_bart_on_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Now you can compare true_labels and predicted_labels to compute accuracy, precision, etc.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Flatten the lists of true and predicted labels for evaluation\u001B[39;00m\n\u001B[0;32m      9\u001B[0m flattened_true \u001B[38;5;241m=\u001B[39m [label \u001B[38;5;28;01mfor\u001B[39;00m sublist \u001B[38;5;129;01min\u001B[39;00m true_labels \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m sublist]\n",
      "Cell \u001B[1;32mIn[39], line 52\u001B[0m, in \u001B[0;36mevaluate_bart_on_dataset\u001B[1;34m(dataset, label_options)\u001B[0m\n\u001B[0;32m     49\u001B[0m true_labels \u001B[38;5;241m=\u001B[39m entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Get BART's generated labels\u001B[39;00m\n\u001B[1;32m---> 52\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m \u001B[43mclassify_text_with_bart\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m predicted_label_names \u001B[38;5;241m=\u001B[39m extract_labels_from_generated_text(generated_text, label_options)\n\u001B[0;32m     54\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m map_labels_to_indices(predicted_label_names, label_options)\n",
      "Cell \u001B[1;32mIn[39], line 10\u001B[0m, in \u001B[0;36mclassify_text_with_bart\u001B[1;34m(text, label_options)\u001B[0m\n\u001B[0;32m      7\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer(prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Generate output from BART\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Decode the generated output\u001B[39;00m\n\u001B[0;32m     13\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Multilingual-Evaluation-of-Generative-AI\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Multilingual-Evaluation-of-Generative-AI\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1874\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1867\u001B[0m         model_kwargs[cache_name] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1868\u001B[0m             DynamicCache\u001B[38;5;241m.\u001B[39mfrom_legacy_cache(past)\n\u001B[0;32m   1869\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m requires_cross_attention_cache\n\u001B[0;32m   1870\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m EncoderDecoderCache\u001B[38;5;241m.\u001B[39mfrom_legacy_cache(past)\n\u001B[0;32m   1871\u001B[0m         )\n\u001B[0;32m   1872\u001B[0m         use_dynamic_cache_by_default \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m-> 1874\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_generated_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_default_max_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1876\u001B[0m \u001B[38;5;66;03m# 7. determine generation mode\u001B[39;00m\n\u001B[0;32m   1877\u001B[0m generation_mode \u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mget_generation_mode(assistant_model)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Multilingual-Evaluation-of-Generative-AI\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1266\u001B[0m, in \u001B[0;36mGenerationMixin._validate_generated_length\u001B[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001B[0m\n\u001B[0;32m   1264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_ids_length \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mmax_length:\n\u001B[0;32m   1265\u001B[0m     input_ids_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1266\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput length of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_ids_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_ids_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but `max_length` is set to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1268\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. This can lead to unexpected behavior. You should consider\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1269\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1270\u001B[0m     )\n\u001B[0;32m   1272\u001B[0m \u001B[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001B[39;00m\n\u001B[0;32m   1273\u001B[0m min_length_error_suffix \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1274\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1275\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincrease the maximum length.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1276\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Input length of input_ids is 902, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30c0667fb9c03eb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
